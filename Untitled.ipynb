{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060be020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3db84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load(\"datasets/mHealth_y_long_tr.npy\")\n",
    "y_seg = np.load(\"datasets/mHealth_y_seg_long_tr.npy\")\n",
    "bound_label = np.load(\"datasets/mHealth_file_boundaries_tr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53d3c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len = 0\n",
    "i = 0\n",
    "for p,c in zip(y,y[1:]):\n",
    "    len+=1\n",
    "    if not p == c:\n",
    "        i+=1\n",
    "    if i==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c3514de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 35173,  68863, 104395, 139775, 175103, 209050, 241255, 275508],\n",
       "       dtype=int64),)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(bound_label != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8100ffdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275509,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_seg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "223c742d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_seg == 1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e14dbaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46075"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3071+6143+9215+12287+15359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266f88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c39e6809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 92, 92, 92])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_seg.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bab010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import BertConfig\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_position=512):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.position_embeddings = nn.Embedding(max_position, embedding_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        seq_length = inputs.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=inputs.device)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        return inputs + position_embeddings\n",
    "    \n",
    "class BERTSegm(nn.Module):\n",
    "    def __init__(self, num_classes, num_features) -> None:\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_json_file(\"bert-tiny.json\")\n",
    "        self.inp = nn.Linear(num_features, 128)\n",
    "        self.encoder = AutoModelForTokenClassification.from_config(config)\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "        self.pos_embed = PositionalEmbedding(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inp(x)\n",
    "        x = self.pos_embed(x)\n",
    "        out = self.encoder.bert.encoder(x)\n",
    "        return self.classifier(out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f722f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BERTSegm(num_classes=12, num_features=23).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e900c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(torch.randn((512,512,23)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "385ab680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2933e-01,  1.4916e-01,  3.9722e-01,  ...,  4.2865e-01,\n",
       "          -3.0290e-02, -5.9482e-01],\n",
       "         [-3.0235e-01, -5.6399e-02, -5.5818e-01,  ...,  2.5147e-01,\n",
       "          -1.1013e-01, -1.0180e+00],\n",
       "         [ 8.0633e-02,  7.5034e-01,  1.5526e+00,  ...,  2.5402e-01,\n",
       "          -3.4136e-01, -2.4892e-01],\n",
       "         ...,\n",
       "         [-2.5613e-01, -3.1860e-01,  1.0217e+00,  ..., -2.5303e-01,\n",
       "          -6.1939e-01, -7.1464e-01],\n",
       "         [-9.7204e-01,  4.2866e-01,  9.7115e-02,  ..., -7.1851e-01,\n",
       "          -1.2944e+00,  2.2697e-01],\n",
       "         [ 1.7860e-01,  5.2658e-01,  5.9124e-01,  ..., -2.6488e-01,\n",
       "          -8.4976e-02, -1.5366e-01]],\n",
       "\n",
       "        [[-2.4464e-01, -6.4345e-02,  5.4099e-01,  ...,  1.2198e-03,\n",
       "          -9.3975e-01, -2.8970e-01],\n",
       "         [-7.3290e-01, -3.2472e-01, -4.6716e-01,  ..., -5.0189e-01,\n",
       "          -4.8248e-01, -2.4773e-01],\n",
       "         [-3.8018e-02,  2.8830e-01,  6.7905e-01,  ...,  6.6741e-02,\n",
       "          -5.4882e-01, -5.4100e-01],\n",
       "         ...,\n",
       "         [-2.3882e-03, -6.5650e-01,  7.2343e-01,  ..., -6.1269e-01,\n",
       "          -5.7460e-01, -7.5118e-01],\n",
       "         [-3.4537e-01,  7.2536e-04,  7.1747e-01,  ...,  6.8048e-02,\n",
       "          -5.2012e-01,  5.8078e-01],\n",
       "         [ 6.9314e-01, -4.4186e-02,  2.0359e-01,  ..., -7.6405e-01,\n",
       "          -1.1880e-02, -7.1820e-01]],\n",
       "\n",
       "        [[-4.9772e-02,  5.6132e-01, -1.7677e-01,  ...,  3.0463e-01,\n",
       "          -4.3729e-01, -4.6307e-01],\n",
       "         [-1.4165e-01,  4.0483e-01,  2.1455e-01,  ..., -5.6209e-01,\n",
       "           4.2068e-01, -3.9688e-01],\n",
       "         [ 2.8148e-01,  2.6207e-01,  6.1402e-01,  ..., -2.0256e-02,\n",
       "          -9.4684e-02, -3.1248e-01],\n",
       "         ...,\n",
       "         [-3.8976e-01, -7.8893e-01,  2.3154e-01,  ...,  2.5418e-01,\n",
       "          -4.7037e-01, -4.0224e-01],\n",
       "         [-2.9975e-01,  1.9854e-01,  3.5005e-01,  ..., -5.3085e-01,\n",
       "          -5.8966e-01,  4.8854e-01],\n",
       "         [ 4.4838e-01, -5.1303e-01,  3.6561e-01,  ..., -6.4088e-01,\n",
       "          -3.3107e-01, -6.0346e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.1449e-01,  1.4184e-01,  7.4666e-01,  ...,  9.6013e-02,\n",
       "          -1.6167e-01, -5.7353e-01],\n",
       "         [-1.6900e-01,  7.6606e-01, -1.3301e-01,  ..., -1.1720e+00,\n",
       "          -1.0893e-01, -7.8060e-01],\n",
       "         [-2.0753e-01,  2.5297e-02,  4.6621e-01,  ...,  2.0570e-01,\n",
       "          -2.5114e-01, -5.4040e-01],\n",
       "         ...,\n",
       "         [-9.7711e-02, -6.3476e-01,  4.5842e-01,  ...,  1.1204e-01,\n",
       "          -1.4322e-01, -3.6200e-01],\n",
       "         [-7.6646e-01, -2.7252e-01,  4.6488e-01,  ..., -3.0186e-01,\n",
       "          -6.5486e-01,  4.8413e-02],\n",
       "         [ 5.2214e-01, -2.0901e-01,  4.0462e-01,  ..., -7.5093e-01,\n",
       "           6.6772e-02, -5.0557e-01]],\n",
       "\n",
       "        [[-1.7466e-01,  2.3530e-01,  5.9804e-01,  ...,  4.4456e-01,\n",
       "          -6.0635e-01, -5.7588e-01],\n",
       "         [-2.7690e-01, -7.9961e-02, -8.3894e-02,  ..., -5.9870e-01,\n",
       "           1.1828e-01, -4.3690e-02],\n",
       "         [ 8.7761e-02,  6.4381e-01,  1.2438e+00,  ...,  5.8620e-02,\n",
       "          -8.3453e-02, -7.7586e-01],\n",
       "         ...,\n",
       "         [-2.7198e-01, -2.6947e-02,  1.1155e+00,  ..., -1.4365e-01,\n",
       "          -3.6102e-01, -3.9449e-01],\n",
       "         [-2.9372e-01, -2.3534e-01, -3.0131e-01,  ..., -3.8575e-01,\n",
       "          -9.9166e-01, -1.6585e-01],\n",
       "         [ 4.0884e-01, -2.6304e-01,  2.8240e-01,  ..., -3.9019e-01,\n",
       "          -2.5873e-01, -5.0292e-01]],\n",
       "\n",
       "        [[-2.9535e-01,  2.9915e-01,  3.2594e-01,  ..., -8.9240e-02,\n",
       "          -9.4464e-01,  3.7627e-01],\n",
       "         [-5.5804e-01,  1.3116e-01, -2.7436e-01,  ..., -5.0333e-01,\n",
       "          -1.8334e-01, -7.0255e-01],\n",
       "         [-9.0484e-02,  9.7508e-01,  8.1243e-01,  ..., -1.2010e-01,\n",
       "          -2.3677e-01, -6.8322e-01],\n",
       "         ...,\n",
       "         [-2.6872e-01,  2.4609e-01,  5.4772e-01,  ..., -1.0930e+00,\n",
       "          -1.8829e-01, -5.0632e-01],\n",
       "         [-4.4715e-01,  6.7014e-02, -1.6154e-01,  ...,  1.5857e-01,\n",
       "          -1.0198e+00, -6.2486e-02],\n",
       "         [ 5.1063e-01, -2.4025e-01,  1.5645e-01,  ..., -6.0106e-01,\n",
       "          -7.9946e-01, -4.2279e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1ab556",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (23) must match the size of tensor b (128) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m23\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mBERTSegm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(out\u001b[38;5;241m.\u001b[39mlast_hidden_state)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mPositionalEmbedding.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     13\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(seq_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     14\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (23) must match the size of tensor b (128) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "m(torch.randn((512,512,23)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b83178a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove a module from the model\n",
    "delattr(mmodel.bert, 'embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e9cb44b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8f8bfebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_json_file(\"bert-tiny.json\")\n",
    "mmodel = AutoModelForTokenClassification.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6fcbc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out = mmodel.bert.embeddings(inputs_embeds=torch.randn((32,512,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "50e9b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSeg(nn.Module):\n",
    "    def __init__(self, num_classes, num_features) -> None:\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_json_file(\"bert-tiny.json\")\n",
    "        #self.inp = nn.Linear(num_features, 128)\n",
    "        self.encoder = AutoModelForTokenClassification.from_config(config)\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "        self.pos_embed = PositionalEmbedding(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_embed(x)\n",
    "        out = self.encoder.bert.encoder(x)\n",
    "        return self.classifier(out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9150677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = mmodel.bert.encoder(in_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "59481a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Linear(128,23)\n",
    "prob = f(o.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "3d886068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "c0274f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([262144])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b029562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = PositionalEmbedding(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "b87acdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2896,  2.0547, -2.1382,  ...,  0.6136,  1.0230,  0.3340],\n",
       "         [ 1.8000,  1.0423, -0.3182,  ...,  0.2610,  0.7704, -1.0579],\n",
       "         [ 0.4798, -1.5702, -0.3248,  ...,  1.4221,  0.2539, -0.4087],\n",
       "         ...,\n",
       "         [-0.1173, -1.9428, -0.2851,  ...,  1.4580,  0.1198,  0.0894],\n",
       "         [-0.9846,  0.2761, -0.4102,  ...,  3.1205, -2.1477, -0.5166],\n",
       "         [-0.4530, -0.5329,  4.1088,  ...,  1.4035,  2.0561,  0.1750]],\n",
       "\n",
       "        [[ 1.2817,  2.5691, -0.1125,  ...,  0.1818,  1.2144, -0.5035],\n",
       "         [ 0.6824, -1.0774,  1.8793,  ...,  1.2554,  0.4793, -2.0228],\n",
       "         [-0.7329, -1.0398,  0.2482,  ...,  1.8106,  2.3121, -0.5784],\n",
       "         ...,\n",
       "         [ 0.4181,  0.8350, -0.1803,  ..., -0.2359,  2.1807,  1.5713],\n",
       "         [-2.6923, -1.8212,  0.6890,  ...,  0.6488, -2.7554, -0.0834],\n",
       "         [-1.5375,  0.5374,  1.3506,  ...,  0.9379,  1.7719,  0.6182]],\n",
       "\n",
       "        [[-0.0953,  2.0021, -1.4846,  ...,  0.6640,  3.1020, -0.7955],\n",
       "         [-0.0656,  1.9750,  0.7166,  ...,  0.2847,  0.4455, -2.5265],\n",
       "         [-0.0886, -0.2569,  0.0587,  ..., -0.3404,  0.9495, -1.6532],\n",
       "         ...,\n",
       "         [ 1.6477, -2.1480, -2.6115,  ...,  2.3004,  1.1880, -0.3198],\n",
       "         [-0.6061, -1.0573, -1.2092,  ..., -1.2427, -0.8912, -1.0965],\n",
       "         [-0.1569, -0.0342,  1.0886,  ...,  0.7603,  1.0066,  0.2848]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.8261,  3.6925, -0.0276,  ...,  0.4713,  2.9360,  1.6576],\n",
       "         [ 2.0703,  1.4019,  0.4907,  ..., -0.2123, -0.3017, -1.4292],\n",
       "         [ 0.1467,  0.3507,  0.0684,  ...,  1.3721,  0.7110, -0.3728],\n",
       "         ...,\n",
       "         [-0.6067,  0.6487, -1.7554,  ...,  2.6214,  0.5397, -1.0886],\n",
       "         [-0.4207, -2.8563, -1.3868,  ..., -0.9418, -1.7956, -0.6269],\n",
       "         [-1.0229,  1.4524,  1.2986,  ...,  2.2588,  1.6169,  0.5871]],\n",
       "\n",
       "        [[ 0.5982,  1.3809, -2.5109,  ...,  1.6847,  0.5103, -0.6059],\n",
       "         [-2.5494,  1.4076,  0.2167,  ..., -1.5893,  0.0567, -1.4867],\n",
       "         [-1.1774, -2.2555,  0.0516,  ...,  1.4804,  0.2639, -0.3836],\n",
       "         ...,\n",
       "         [ 1.0814, -0.0280,  0.1836,  ...,  0.1646,  1.9570,  1.3172],\n",
       "         [-1.9093,  0.2133,  0.5695,  ..., -1.0525, -1.4565,  0.6432],\n",
       "         [-0.4559,  1.9899,  2.2024,  ...,  0.1410,  2.1456,  0.8290]],\n",
       "\n",
       "        [[ 1.9540,  2.4918, -1.3996,  ...,  0.5220,  2.4166, -0.6213],\n",
       "         [-0.1239,  0.0408, -0.0947,  ...,  0.1432, -1.5111,  0.3500],\n",
       "         [ 0.3803, -0.6225,  0.4696,  ..., -0.0937,  0.1523, -0.2423],\n",
       "         ...,\n",
       "         [ 0.2654,  0.6563, -0.6119,  ..., -0.4248,  1.1294,  0.3832],\n",
       "         [-0.9510, -1.5592, -1.4496,  ...,  0.1413, -0.0510, -0.8357],\n",
       "         [-1.4860,  1.1248,  1.7976,  ..., -0.8424,  1.8987, -0.1588]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos(torch.randn((32,512,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7f44f216",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [189]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1758\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1756\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1768\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1770\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1772\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py:984\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    981\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length)), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 984\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    985\u001b[0m         buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m    986\u001b[0m         buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m buffered_token_type_ids\u001b[38;5;241m.\u001b[39mexpand(batch_size, seq_length)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'embeddings'"
     ]
    }
   ],
   "source": [
    "mmodel(inputs_embeds=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93639c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2aa4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a83c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ed3f2ec2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [176]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [172]\u001b[0m, in \u001b[0;36mBERTSeg.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#x = self.inp(x)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1758\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1756\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1768\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1770\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1772\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py:974\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 974\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m    975\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "m(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ecc15ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(512, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.encoder.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "222e0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "del m.encoder.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0c5e68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.randn((32,512,23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e2411986",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = PositionalEmbedding(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "86179245",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.position_embeddings.weight = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "52a9a1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.4320e+00,  2.1988e-01,  1.7426e+00,  ..., -1.7832e+00,\n",
       "          -6.3533e-01,  9.6177e-01],\n",
       "         [ 1.8185e-01,  2.5144e-01, -2.4838e+00,  ...,  7.8671e-02,\n",
       "           1.4357e+00, -5.9425e-01],\n",
       "         [ 3.4595e-01,  1.0057e+00, -9.0923e-01,  ..., -1.6275e-01,\n",
       "          -1.4897e+00,  3.2884e-01],\n",
       "         ...,\n",
       "         [-1.2382e+00,  1.0645e+00, -7.8116e-01,  ...,  6.5564e-01,\n",
       "          -9.1948e-01,  2.2079e-01],\n",
       "         [ 5.9322e-02, -2.1473e+00,  1.4042e+00,  ..., -1.3013e+00,\n",
       "          -1.1026e+00, -7.6612e-01],\n",
       "         [-4.9983e-01, -2.6151e+00,  1.1801e-02,  ...,  1.6513e+00,\n",
       "          -2.3155e+00, -4.4420e-01]],\n",
       "\n",
       "        [[-8.6146e-01,  1.9651e-02, -1.5289e+00,  ...,  7.9150e-01,\n",
       "          -1.3188e-01, -1.2547e+00],\n",
       "         [-2.0090e-03,  1.1216e+00, -1.0643e+00,  ..., -1.2473e+00,\n",
       "           3.0794e-01,  6.3661e-01],\n",
       "         [ 1.0247e+00, -2.3116e+00, -1.0727e+00,  ...,  8.9503e-01,\n",
       "           3.1898e+00,  1.5610e-01],\n",
       "         ...,\n",
       "         [ 1.7303e+00, -2.2887e+00,  6.0558e-01,  ..., -1.0133e+00,\n",
       "          -4.8750e-01, -1.3624e+00],\n",
       "         [ 1.1808e+00, -1.1866e+00, -1.2559e+00,  ..., -2.1180e+00,\n",
       "           9.5694e-02, -1.1076e+00],\n",
       "         [-4.2260e-02,  1.8820e+00,  1.6091e+00,  ..., -5.3343e-01,\n",
       "           5.9224e-01,  7.7961e-01]],\n",
       "\n",
       "        [[-3.2946e-01,  7.1446e-01,  1.4180e+00,  ..., -1.0702e+00,\n",
       "          -4.8307e-01, -2.7363e-01],\n",
       "         [ 4.4865e-01,  2.1409e-01,  1.1792e+00,  ...,  1.2331e+00,\n",
       "          -8.9285e-02, -5.9490e-01],\n",
       "         [-2.4072e+00,  2.3339e-01,  1.1621e+00,  ...,  9.5473e-01,\n",
       "          -1.8386e+00, -3.9801e-01],\n",
       "         ...,\n",
       "         [-1.1590e-01,  5.2544e-01,  1.5696e+00,  ...,  9.6041e-01,\n",
       "          -1.5467e+00, -2.3263e+00],\n",
       "         [ 3.9392e-01,  1.1163e+00,  2.8424e-01,  ...,  1.5369e-01,\n",
       "          -1.9033e+00,  9.9767e-01],\n",
       "         [-1.4441e+00, -7.7681e-01,  2.5671e-01,  ..., -4.3159e-01,\n",
       "          -2.2340e-03,  1.9734e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.3825e+00,  5.5015e-01, -6.0392e-01,  ..., -7.1900e-01,\n",
       "           3.0866e-01, -2.5662e-01],\n",
       "         [-1.4524e+00,  9.9556e-01,  1.3663e+00,  ...,  6.8764e-01,\n",
       "           5.6733e-01,  1.5895e+00],\n",
       "         [ 1.4185e+00,  5.0243e-01, -7.7057e-01,  ...,  8.3349e-01,\n",
       "           2.7632e+00, -9.3961e-01],\n",
       "         ...,\n",
       "         [ 8.0089e-01, -5.1640e-01, -1.2890e+00,  ..., -5.2210e-01,\n",
       "          -3.0040e-01, -1.3641e+00],\n",
       "         [ 3.0397e-01, -1.8378e-01, -1.7413e+00,  ..., -1.0607e+00,\n",
       "          -1.9211e+00, -6.6612e-01],\n",
       "         [ 9.6807e-01,  1.3632e+00, -2.9283e-01,  ...,  2.0115e+00,\n",
       "          -1.1929e-01, -1.8803e+00]],\n",
       "\n",
       "        [[-7.1486e-02,  7.1349e-01,  3.5140e-01,  ..., -7.8797e-01,\n",
       "          -4.7344e-01,  5.8125e-01],\n",
       "         [ 2.0013e+00, -1.2850e+00, -8.3843e-01,  ..., -1.3820e+00,\n",
       "           1.1248e+00, -6.0537e-01],\n",
       "         [-2.0523e+00, -1.2078e-01,  1.9870e-01,  ...,  4.2409e-01,\n",
       "           6.3750e-01,  6.2789e-01],\n",
       "         ...,\n",
       "         [-3.3815e-01,  1.0488e-01,  4.4282e-01,  ..., -3.3260e-01,\n",
       "           9.2288e-01, -8.0238e-01],\n",
       "         [ 2.5072e-01, -5.6762e-02, -3.1870e-01,  ..., -8.8547e-01,\n",
       "          -1.1411e+00, -2.2440e-01],\n",
       "         [-6.0995e-01, -8.6536e-01,  5.2212e-01,  ...,  5.4045e-01,\n",
       "           1.1470e+00, -3.1621e-01]],\n",
       "\n",
       "        [[ 7.6886e-01, -1.7366e+00, -1.6889e-01,  ..., -1.9385e-01,\n",
       "           1.3468e-01, -6.6265e-01],\n",
       "         [ 1.6354e+00, -1.6477e+00, -4.2634e-01,  ...,  9.2805e-01,\n",
       "           1.0213e-01, -5.0457e-01],\n",
       "         [ 9.7347e-01,  2.8613e-01, -7.0924e-01,  ..., -1.3798e-01,\n",
       "          -6.9299e-01,  1.7441e+00],\n",
       "         ...,\n",
       "         [ 8.2764e-01,  9.0576e-02, -2.0635e+00,  ...,  1.1698e+00,\n",
       "           1.5842e+00, -1.1268e+00],\n",
       "         [ 1.1531e+00,  2.5158e-01, -6.2987e-01,  ...,  1.2583e+00,\n",
       "          -7.4606e-01,  4.2829e-01],\n",
       "         [-2.3799e-01,  2.1118e+00, -1.2677e+00,  ...,  1.0862e+00,\n",
       "           8.6431e-01,  1.6018e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8626b78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512, 128])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "550a4c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.6718,  0.2423,  1.9306,  ..., -2.0007, -0.7200,  1.0869],\n",
       "         [ 0.1931,  0.3135, -2.6411,  ...,  0.1278,  1.5812, -0.5818],\n",
       "         [ 0.3856,  1.2189, -1.0712,  ..., -0.0000, -1.7473,  0.4304],\n",
       "         ...,\n",
       "         [-1.2719,  1.3967, -0.0000,  ...,  0.0000, -0.8657,  0.4507],\n",
       "         [ 0.0963, -2.0640,  1.4662,  ..., -1.2195, -1.0253, -0.6729],\n",
       "         [-0.5476, -2.6904,  0.0097,  ...,  1.7199, -2.3850, -0.0000]],\n",
       "\n",
       "        [[-0.9846,  0.0922, -1.7307,  ...,  0.9921, -0.0000, -1.3791],\n",
       "         [-0.0265,  1.2129, -1.1264,  ..., -1.3051,  0.0000,  0.7105],\n",
       "         [ 1.0954, -0.0000, -1.1666,  ...,  1.0001,  3.5033,  0.2061],\n",
       "         ...,\n",
       "         [ 1.7592, -2.5936,  0.5582,  ..., -1.1976, -0.6266, -1.5661],\n",
       "         [ 1.2185, -1.2963, -1.3873,  ..., -0.0000,  0.0862, -0.0000],\n",
       "         [-0.1666,  2.1264,  1.7908,  ..., -0.6898,  0.6178,  0.8556]],\n",
       "\n",
       "        [[-0.2881,  0.7473,  1.4036,  ..., -0.0000, -0.3980, -0.1826],\n",
       "         [ 0.2898,  0.0000,  1.0671,  ...,  1.1377, -0.0000, -0.7241],\n",
       "         [-0.0000,  0.2164,  0.0000,  ...,  0.9757, -1.9690, -0.4352],\n",
       "         ...,\n",
       "         [-0.2604,  0.5091,  0.0000,  ...,  0.9985, -1.8275, -2.6862],\n",
       "         [ 0.3219,  1.2813,  0.0000,  ...,  0.0738, -2.5113,  1.1480],\n",
       "         [-1.9145, -1.0125,  0.2819,  ..., -0.5739, -0.0335,  0.2412]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.4479,  0.5062, -0.8874,  ..., -1.0068,  0.2134, -0.4408],\n",
       "         [-1.6016,  0.9779,  1.3464,  ...,  0.6589,  0.5300,  1.6058],\n",
       "         [ 1.4952,  0.4590, -1.0686,  ...,  0.8516,  0.0000, -1.2367],\n",
       "         ...,\n",
       "         [ 1.1737, -0.3757, -1.3327,  ..., -0.3826, -0.1182, -1.3909],\n",
       "         [ 0.2670, -0.1979, -1.8396,  ..., -1.1135, -2.0162, -0.6886],\n",
       "         [ 0.8817,  0.0000, -0.0000,  ...,  2.0666, -0.0000, -2.1666]],\n",
       "\n",
       "        [[-0.0804,  0.8285,  0.4144,  ..., -0.8207, -0.4798,  0.0000],\n",
       "         [ 2.2199, -1.5909, -1.0838,  ..., -1.7049,  1.2351, -0.7779],\n",
       "         [-1.9897, -0.0895,  0.0000,  ...,  0.4350,  0.0000,  0.6431],\n",
       "         ...,\n",
       "         [-0.5278,  0.0474,  0.4300,  ..., -0.4709,  1.0115, -1.0128],\n",
       "         [ 0.2830, -0.0093, -0.3147,  ..., -0.9233, -1.2098, -0.1805],\n",
       "         [-0.9816, -0.0000,  0.4780,  ...,  0.5198,  1.2781, -0.5429]],\n",
       "\n",
       "        [[ 0.9211, -1.7896, -0.0806,  ..., -0.0917,  0.2653, -0.5940],\n",
       "         [ 1.6765, -1.7660, -0.4844,  ...,  0.9701,  0.0884, -0.5385],\n",
       "         [ 0.9262,  0.2333, -0.8522,  ..., -0.2224, -0.8232,  1.8131],\n",
       "         ...,\n",
       "         [ 0.7591,  0.0405, -2.2034,  ...,  1.1570,  1.5813, -1.2061],\n",
       "         [ 1.0738,  0.1307, -0.8534,  ...,  1.2360, -0.9692,  0.3383],\n",
       "         [-0.4133,  2.2660, -1.5357,  ...,  1.1172,  0.8641,  1.7087]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "93146ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.encoder.bert.embeddings.word_embeddings = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2cd51732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 0.1880, -0.6586, -0.7606,  ...,  0.2042,  0.2719, -1.0301],\n",
       "         [ 0.5953, -0.5369, -0.8484,  ...,  1.2117,  0.9829, -0.1000],\n",
       "         [ 0.6173,  0.1903, -0.2402,  ..., -0.4737,  0.2069, -0.2843],\n",
       "         ...,\n",
       "         [ 1.3340, -0.4366,  0.6401,  ...,  1.1678,  0.6808,  0.2933],\n",
       "         [ 0.5613,  0.2143,  0.2366,  ...,  0.1705,  0.7172, -0.3656],\n",
       "         [ 0.5741,  0.8158, -1.1151,  ..., -0.8200, -0.3243, -0.1889]],\n",
       "\n",
       "        [[ 0.0429, -0.9543,  0.3906,  ...,  0.5439,  0.9809,  1.8685],\n",
       "         [-0.4763,  1.4584,  0.2234,  ..., -0.2544,  0.5315,  0.8301],\n",
       "         [-0.7462, -0.1996,  0.1201,  ...,  1.2419,  0.4502,  0.7334],\n",
       "         ...,\n",
       "         [-0.0692,  0.2254,  0.3808,  ...,  0.1006, -0.1023,  0.1586],\n",
       "         [-0.0297,  0.3490,  0.1799,  ..., -0.2135,  0.2754,  0.9241],\n",
       "         [ 0.8255, -1.1712, -0.0334,  ...,  0.7247,  0.7107,  1.3413]],\n",
       "\n",
       "        [[ 0.9637, -0.7080, -0.7318,  ..., -0.1645,  0.2158,  0.1161],\n",
       "         [ 0.5350, -0.2402, -0.3280,  ...,  0.1654,  0.3043, -0.6707],\n",
       "         [ 1.5629,  0.0519,  0.2364,  ..., -0.3157, -0.2068, -1.4217],\n",
       "         ...,\n",
       "         [-0.9091,  0.4097, -0.7283,  ..., -0.3945, -0.3066,  0.2634],\n",
       "         [-0.5872, -0.1854, -1.2981,  ...,  0.5911,  0.2622, -0.8273],\n",
       "         [ 1.9967,  0.4884,  0.0401,  ...,  0.2465, -0.2738,  0.2360]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.5156, -0.2759,  0.2943,  ..., -0.0273,  0.2030,  0.6565],\n",
       "         [ 0.5554,  0.2536, -0.0703,  ...,  0.7013,  0.6311, -0.2455],\n",
       "         [ 0.4387,  0.1588,  0.4523,  ...,  0.1414,  0.5849, -0.8099],\n",
       "         ...,\n",
       "         [ 0.6658,  0.1653, -0.5622,  ...,  0.3527, -0.9211,  0.6506],\n",
       "         [ 0.5726,  0.8170,  1.2892,  ..., -0.3672,  0.3592,  0.2354],\n",
       "         [ 0.0988,  0.8915, -0.3662,  ...,  0.0057, -0.4887,  0.1336]],\n",
       "\n",
       "        [[ 0.0300,  1.5420, -0.6710,  ..., -0.1446, -0.9700, -0.5450],\n",
       "         [ 0.4120, -0.7553, -0.8896,  ..., -0.1183,  0.4842,  0.0855],\n",
       "         [ 2.1331, -0.7705, -0.1876,  ..., -0.6526,  0.3041, -0.8321],\n",
       "         ...,\n",
       "         [ 0.0642,  0.3185, -0.8019,  ..., -0.1600,  0.9170, -0.6868],\n",
       "         [ 1.1767,  0.6303, -0.0888,  ..., -0.6140, -0.7647, -0.3699],\n",
       "         [-0.0391,  0.7363,  0.3667,  ...,  0.2393,  0.6641, -0.4235]],\n",
       "\n",
       "        [[ 0.9407,  0.7156, -0.7011,  ..., -0.1058, -0.4961,  0.8422],\n",
       "         [ 0.1325,  0.2161, -0.7640,  ...,  0.8730,  0.1704,  0.6059],\n",
       "         [ 1.2600,  0.2655,  0.2605,  ...,  0.2185, -0.3266, -0.1547],\n",
       "         ...,\n",
       "         [ 0.0780, -0.6569,  0.6395,  ...,  0.4408,  1.3820, -1.1406],\n",
       "         [ 1.0701,  0.0512, -0.0265,  ...,  0.0714,  0.0476, -0.0371],\n",
       "         [-0.0995,  0.0416, -0.8521,  ...,  0.3156,  0.0569, -0.3248]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "715883ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 0.3065, -0.6531,  0.6054,  ..., -0.7876, -0.3967,  0.2144],\n",
       "         [-0.2096, -1.3280, -0.2990,  ..., -0.1703, -0.9313,  0.5488],\n",
       "         [-0.4618, -0.0162, -0.4783,  ..., -0.6737, -1.0046,  0.3024],\n",
       "         ...,\n",
       "         [-0.4057, -0.0544, -0.5795,  ...,  0.5137, -0.2792,  1.2252],\n",
       "         [-0.7764, -0.1338, -0.0584,  ...,  0.7757, -0.4042,  0.5364],\n",
       "         [ 0.3115, -1.6288,  0.2534,  ..., -0.3534, -0.3572, -0.8612]],\n",
       "\n",
       "        [[-0.6044,  1.0846,  0.7650,  ..., -0.2340,  0.2806, -0.5960],\n",
       "         [ 0.3370,  0.1588, -0.7963,  ...,  0.2715,  0.0114,  0.0974],\n",
       "         [ 0.2547,  0.2627, -0.4987,  ...,  1.4426, -0.3163,  0.2099],\n",
       "         ...,\n",
       "         [-1.9444,  0.2421, -1.4691,  ...,  1.1536,  0.6275, -1.0570],\n",
       "         [ 0.2851,  0.3631, -0.0201,  ..., -0.3419, -0.7592,  0.1414],\n",
       "         [-1.5581, -0.0648, -0.1921,  ...,  1.0379, -0.1463, -0.1475]],\n",
       "\n",
       "        [[-0.0220, -0.5001,  1.0301,  ...,  1.1702,  0.2251, -0.2296],\n",
       "         [-0.3695, -1.3319, -0.1474,  ...,  0.8531, -0.4941,  0.0625],\n",
       "         [-0.4534, -0.7873,  0.6166,  ..., -0.0940, -0.0995,  0.5242],\n",
       "         ...,\n",
       "         [ 0.3675,  0.3938, -1.2164,  ..., -0.0501,  0.4140, -0.5487],\n",
       "         [ 0.1486, -0.2543, -0.6420,  ..., -0.3820,  0.2343,  0.3268],\n",
       "         [-0.7178,  0.1941, -0.1357,  ...,  0.6913, -0.2397, -0.3869]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4958, -0.5182, -0.7209,  ...,  0.4621, -0.3684, -0.6118],\n",
       "         [-0.4125,  0.4287, -0.3692,  ...,  0.4470,  0.7302, -1.6950],\n",
       "         [-1.6844, -0.4006,  0.1047,  ...,  0.0759, -0.2116,  0.3621],\n",
       "         ...,\n",
       "         [-0.2283, -0.4573, -0.1553,  ...,  1.2623, -0.2992,  0.1366],\n",
       "         [-0.1566,  0.4213,  0.3242,  ..., -0.0640, -0.3375, -0.1244],\n",
       "         [-0.0297, -0.0623,  1.3509,  ...,  0.6628, -0.7378, -0.1604]],\n",
       "\n",
       "        [[ 0.6578, -1.3292, -0.4294,  ..., -0.0269, -0.5102,  0.4747],\n",
       "         [-0.5881, -0.6699, -0.1468,  ...,  0.5836,  0.0195,  0.3735],\n",
       "         [ 0.3754, -0.3372,  0.1235,  ..., -0.0640,  0.3184,  1.0773],\n",
       "         ...,\n",
       "         [ 0.3343, -0.4190,  0.2681,  ..., -0.3910, -0.1895, -0.4848],\n",
       "         [ 0.2894, -0.4268, -1.0414,  ...,  1.1285, -0.5801, -0.3072],\n",
       "         [ 1.0687, -0.6791, -0.0990,  ..., -0.5927,  0.0587,  0.6854]],\n",
       "\n",
       "        [[-0.0076,  0.1150,  0.4050,  ...,  0.9048, -0.1936,  0.4749],\n",
       "         [-0.9848, -1.2375,  0.0223,  ...,  0.8296, -0.5947,  0.6863],\n",
       "         [-0.9771, -0.8450,  0.9445,  ...,  1.1579, -0.3883,  0.3072],\n",
       "         ...,\n",
       "         [-0.6221, -0.5676, -0.6880,  ..., -0.1094,  0.0387, -0.1001],\n",
       "         [ 0.0721, -0.0643, -0.4409,  ..., -0.5474,  0.1230, -0.1390],\n",
       "         [-0.5607, -0.1737, -1.1301,  ...,  0.4180,  0.4582, -0.5222]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aff61197",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Linear(12,456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08fe1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.sparse import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "96b969e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Embedding(2,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1f14811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.6718,  0.2423,  1.9306,  ..., -2.0007, -0.7200,  1.0869],\n",
       "         [ 0.1931,  0.3135, -2.6411,  ...,  0.1278,  1.5812, -0.5818],\n",
       "         [ 0.3856,  1.2189, -1.0712,  ..., -0.1677, -1.7473,  0.4304],\n",
       "         ...,\n",
       "         [-1.2719,  1.3967, -0.7205,  ...,  0.9314, -0.0000,  0.4507],\n",
       "         [ 0.0963, -2.0640,  1.4662,  ..., -1.2195, -1.0253, -0.6729],\n",
       "         [-0.5476, -2.6904,  0.0097,  ...,  1.7199, -2.3850, -0.4335]],\n",
       "\n",
       "        [[-0.9846,  0.0922, -1.7307,  ...,  0.9921, -0.0000, -1.3791],\n",
       "         [-0.0265,  1.2129, -1.1264,  ..., -1.3051,  0.3436,  0.7105],\n",
       "         [ 1.0954, -2.5042, -1.1666,  ...,  1.0001,  3.5033,  0.2061],\n",
       "         ...,\n",
       "         [ 1.7592, -2.5936,  0.5582,  ..., -1.1976, -0.0000, -1.5661],\n",
       "         [ 1.2185, -1.2963, -0.0000,  ..., -2.3038,  0.0862, -1.1974],\n",
       "         [-0.1666,  2.1264,  1.7908,  ..., -0.6898,  0.6178,  0.8556]],\n",
       "\n",
       "        [[-0.2881,  0.7473,  0.0000,  ..., -0.9537, -0.3980, -0.1826],\n",
       "         [ 0.2898,  0.0928,  1.0671,  ...,  1.1377, -0.2226, -0.7241],\n",
       "         [-2.6078,  0.2164,  1.1782,  ...,  0.9757, -1.9690, -0.4352],\n",
       "         ...,\n",
       "         [-0.2604,  0.5091,  1.6673,  ...,  0.9985, -1.8275, -2.6862],\n",
       "         [ 0.3219,  0.0000,  0.2189,  ...,  0.0738, -2.5113,  1.1480],\n",
       "         [-1.9145, -1.0125,  0.2819,  ..., -0.5739, -0.0335,  0.2412]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.4479,  0.5062, -0.8874,  ..., -1.0068,  0.2134, -0.4408],\n",
       "         [-1.6016,  0.9779,  1.3464,  ...,  0.6589,  0.5300,  1.6058],\n",
       "         [ 1.4952,  0.0000, -1.0686,  ...,  0.8516,  3.1354, -1.2367],\n",
       "         ...,\n",
       "         [ 1.1737, -0.3757, -1.3327,  ..., -0.3826, -0.1182, -1.3909],\n",
       "         [ 0.0000, -0.1979, -0.0000,  ..., -1.1135, -2.0162, -0.6886],\n",
       "         [ 0.8817,  1.3593, -0.4642,  ...,  2.0666, -0.2630, -2.1666]],\n",
       "\n",
       "        [[-0.0804,  0.8285,  0.4144,  ..., -0.8207, -0.4798,  0.6968],\n",
       "         [ 2.2199, -0.0000, -1.0838,  ..., -1.7049,  1.2351, -0.7779],\n",
       "         [-1.9897, -0.0895,  0.2037,  ...,  0.0000,  0.6364,  0.6431],\n",
       "         ...,\n",
       "         [-0.5278,  0.0000,  0.4300,  ..., -0.4709,  1.0115, -1.0128],\n",
       "         [ 0.2830, -0.0093, -0.3147,  ..., -0.9233, -1.2098, -0.0000],\n",
       "         [-0.9816, -1.2497,  0.4780,  ...,  0.5198,  1.2781, -0.5429]],\n",
       "\n",
       "        [[ 0.0000, -1.7896, -0.0806,  ..., -0.0917,  0.2653, -0.5940],\n",
       "         [ 1.6765, -1.7660, -0.4844,  ...,  0.9701,  0.0884, -0.5385],\n",
       "         [ 0.9262,  0.2333, -0.8522,  ..., -0.2224, -0.8232,  1.8131],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0405, -2.2034,  ...,  1.1570,  1.5813, -1.2061],\n",
       "         [ 1.0738,  0.1307, -0.8534,  ...,  1.2360, -0.9692,  0.0000],\n",
       "         [-0.4133,  2.2660, -1.5357,  ...,  1.1172,  0.8641,  1.7087]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(inputs_embeds=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7b83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
